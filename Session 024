```
## 2.4. Predicting Price with Size, Location, and Neighborhood

In the final lesson for this project, we're going to try to use all the features 
in our dataset to improve our model. This means that we'll have to do a more careful 
cleaning of the dataset and consider some of the finer points of linear models.

import warnings
from glob import glob

import pandas as pd
import seaborn as sns # Added in this lession 
import wqet_grader
from category_encoders import OneHotEncoder
from IPython.display import VimeoVideo
# For making dashboards:
from ipywidgets import Dropdown, FloatSlider, IntSlider, interact 
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, Ridge  # noqa F401
from sklearn.metrics import mean_absolute_error
from sklearn.pipeline import make_pipeline
from sklearn.utils.validation import check_is_fitted

warnings.simplefilter(action="ignore", category=FutureWarning)
wqet_grader.init("Project 2 Assessment")

### Prepare Data
def wrangle(filepath):
    # Read CSV file
    df = pd.read_csv(filepath)

    # Subset data: Apartments in "Capital Federal", less than 400,000
    mask_ba = df["place_with_parent_names"].str.contains("Capital Federal")
    mask_apt = df["property_type"] == "apartment"
    mask_price = df["price_aprox_usd"] < 400_000
    df = df[mask_ba & mask_apt & mask_price]

    # Subset data: Remove outliers for "surface_covered_in_m2"
    low, high = df["surface_covered_in_m2"].quantile([0.1, 0.9])
    mask_area = df["surface_covered_in_m2"].between(low, high)
    df = df[mask_area]

    # Split "lat-lon" column
    df[["lat", "lon"]] = df["lat-lon"].str.split(",", expand=True).astype(float)
    df.drop(columns="lat-lon", inplace=True)

    # Get place name
    df["neighborhood"] = df["place_with_parent_names"].str.split("|", expand=True)[3]
    df.drop(columns="place_with_parent_names", inplace=True)
    ## Categories with hi and low items
    df.drop(columns=["floor","expenses"],inplace = True)
    df.drop(columns =["operation","property_type","currency","properati_url"],inplace = True)

    
    return df
    
   ####  Task 2.4.1: Use glob to create a list that contains the filenames for all the 
    Buenos Aires real estate CSV files in the data directory. 
    Assign this list to the variable name files.
    
    files = glob("data/buenos-aires-real-estate-*.csv")
 
    Task 2.4.2: Use your wrangle function in a list comprehension to create a list named frames
    
    frames = []
    for file in files:
    df = wrangle(file)
    frames.append(df)
    
    * To Here:
    frames = [wrangle(file)for file in files]
    frames[0].head()
    
  #### ask 2.4.3: Use pd.concat to concatenate it items in frames
  frames = [wrangle(file)for file in files]
   df = pd.concat(frames,ignore_index = True)
  print(df.info())
  df.head()
  
  Or this:
  
  df = pd.concat([wrangle(file)for file in files],ignore_index = True)
print(df.info())
df.head()

### Explore
#### The first thing we need to consider when trying to use all the features df is missing values.
Task 2.4.4: Modify your wrangle function to drop any columns that are more than half NaN values.

df.ianull().sum()/len(df)

Add to function:
df.drop(columns=["floor","expenses"],inplace = True)

The next thing we need to look out for are categorical columns with low or high cardinality. 
#### Task 2.4.5: Calculate the number of unique values for each non-numeric feature in df.

df.select_dtypes("object").nunique()

Add to Wrangle function:
df.select_dtypes("object").nunique()
df.drop(columns =["operation","property_type","currency","properati_url"])
```
