```
## 2.4. Predicting Price with Size, Location, and Neighborhood

In the final lesson for this project, we're going to try to use all the features 
in our dataset to improve our model. This means that we'll have to do a more careful 
cleaning of the dataset and consider some of the finer points of linear models.

import warnings
from glob import glob

import pandas as pd
import seaborn as sns # Added in this lession 
import wqet_grader
from category_encoders import OneHotEncoder
from IPython.display import VimeoVideo
# For making dashboards:
from ipywidgets import Dropdown, FloatSlider, IntSlider, interact 
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, Ridge  # noqa F401
from sklearn.metrics import mean_absolute_error
from sklearn.pipeline import make_pipeline
from sklearn.utils.validation import check_is_fitted

warnings.simplefilter(action="ignore", category=FutureWarning)
wqet_grader.init("Project 2 Assessment")

### Prepare Data
def wrangle(filepath):
    # Read CSV file
    df = pd.read_csv(filepath)

    # Subset data: Apartments in "Capital Federal", less than 400,000
    mask_ba = df["place_with_parent_names"].str.contains("Capital Federal")
    mask_apt = df["property_type"] == "apartment"
    mask_price = df["price_aprox_usd"] < 400_000
    df = df[mask_ba & mask_apt & mask_price]

    # Subset data: Remove outliers for "surface_covered_in_m2"
    low, high = df["surface_covered_in_m2"].quantile([0.1, 0.9])
    mask_area = df["surface_covered_in_m2"].between(low, high)
    df = df[mask_area]

    # Split "lat-lon" column
    df[["lat", "lon"]] = df["lat-lon"].str.split(",", expand=True).astype(float)
    df.drop(columns="lat-lon", inplace=True)

    # Get place name
    df["neighborhood"] = df["place_with_parent_names"].str.split("|", expand=True)[3]
    df.drop(columns="place_with_parent_names", inplace=True)
    ## Categories with hi and low items
    df.drop(columns=["floor","expenses"],inplace = True)
    df.drop(columns =["operation","property_type","currency","properati_url"],inplace = True)

    
    return df
    
   ####  Task 2.4.1: Use glob to create a list that contains the filenames for all the 
    Buenos Aires real estate CSV files in the data directory. 
    Assign this list to the variable name files.
    
    files = glob("data/buenos-aires-real-estate-*.csv")
 
    Task 2.4.2: Use your wrangle function in a list comprehension to create a list named frames
    
    frames = []
    for file in files:
    df = wrangle(file)
    frames.append(df)
    
    * To Here:
    frames = [wrangle(file)for file in files]
    frames[0].head()
    
  #### ask 2.4.3: Use pd.concat to concatenate it items in frames
  frames = [wrangle(file)for file in files]
   df = pd.concat(frames,ignore_index = True)
  print(df.info())
  df.head()
  
  Or this:
  
  df = pd.concat([wrangle(file)for file in files],ignore_index = True)
print(df.info())
df.head()

### Explore
#### The first thing we need to consider when trying to use all the features df is missing values.
Task 2.4.4: Modify your wrangle function to drop any columns that are more than half NaN values.

df.ianull().sum()/len(df)

Add to function:
df.drop(columns=["floor","expenses"],inplace = True)

The next thing we need to look out for are categorical columns with low or high cardinality. 
#### Task 2.4.5: Calculate the number of unique values for each non-numeric feature in df.

df.select_dtypes("object").nunique()

Add to Wrangle function:
df.select_dtypes("object").nunique()
df.drop(columns =["operation","property_type","currency","properati_url"], inplace = True)

### Task 2.4.7: Modify your wrangle function to drop any features that would constitute leakage.

Task 2.4.8: Plot a correlation heatmap of the remaining numerical features in df. 
Since "price_aprox_usd" will be your target, you don't need to include it in your heatmap.

corr =df.select_dtypes("number").drop(columns ="price_aprox_usd").corr()
sns.heatmap(corr)


#### Task 2.4.9: Modify your wrangle function to remove columns so that there are no strongly correlated features in your feature matrix.

df.drop(columns =["surface_total_in_m2","rooms"],inplace = True)
corr =df.select_dtypes("number").corr()
sns.heatmap(corr)

#### Task 2.4.10: Create your feature matrix X_train and target vector y_train. Your target is "price_aprox_usd". 

From Forum:
target = "price_aprox_usd"
features = ["surface_covered_in_m2", "lat", "lon", "neighborhood"]
X_train=df[features]
y_train =df[target]

#### Task 2.4.11: Calculate the baseline mean absolute error for your model.

y_mean = y_train.mean()
y_pred_baseline = [y_mean] * len(y_train)
print("Mean apt price:",y_mean)
print("Baseline MAE:", mean_absolute_error(y_train,y_pred_baseline))

#### Task 2.4.12: Create a pipeline named model that contains a OneHotEncoder, SimpleImputer, and Ridge predictor.
model.fit(X_train,y_train)
ohe = OneHotEncoder(use_cat_names = True)
ohe.fit(X_train)
XT_train = ohe.transform(X_train)
print(XT_train.shape)
XT_train.head()

model = make_pipeline(
    OneHotEncoder(use_cat_names=True),
    LinearRegression()
)
model.fit(X_train,y_train)
